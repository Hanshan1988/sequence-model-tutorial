{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XQ6NsIuDtgr"
   },
   "source": [
    "# Illustrated: Self-Attention\n",
    "Step-by-step guide to self-attention with illustrations and code\n",
    "\n",
    "[medium article](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)\n",
    "\n",
    "[Article author](https://towardsdatascience.com/@remykarem)\n",
    "\n",
    "> Colab made by: [Manuel Romero](https://twitter.com/mrm8488)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U76qWlrbOmx7"
   },
   "source": [
    "![texto alternativo](https://miro.medium.com/max/1973/1*_92bnsMJy8Bl539G4v93yg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOkXKd60Q_Iu"
   },
   "source": [
    "What do *BERT, RoBERTa, ALBERT, SpanBERT, DistilBERT, SesameBERT, SemBERT, MobileBERT, TinyBERT and CamemBERT* all have in common? And Iâ€™m not looking for the answer â€œBERTâ€ ðŸ¤­.\n",
    "Answer: **self-attention** ðŸ¤—. We are not only talking about architectures bearing the name â€œBERTâ€™, but more correctly **Transformer-based architectures**. Transformer-based architectures, which are primarily used in modelling language understanding tasks, eschew the use of recurrence in neural network (RNNs) and instead trust entirely on self-attention mechanisms to draw global dependencies between inputs and outputs. But whatâ€™s the math behind this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yozzTBjBRbAA"
   },
   "source": [
    "The main content of this kernel is to walk you through the mathematical operations involved in a self-attention module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atUYzU3TSD9z"
   },
   "source": [
    "### Step 0. What is self-attention?\n",
    "\n",
    "If youâ€™re thinking if self-attention is similar to attention, then the answer is yes! They fundamentally share the same concept and many common mathematical operations.\n",
    "A self-attention module takes in n inputs, and returns n outputs. What happens in this module? In laymanâ€™s terms, the self-attention mechanism allows the inputs to interact with each other (â€œselfâ€) and find out who they should pay more attention to (â€œattentionâ€). The outputs are aggregates of these interactions and attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDMmHAaSTE6P"
   },
   "source": [
    "Following, we are going to explain and implement:\n",
    "- Prepare inputs\n",
    "- Initialise weights\n",
    "- Derive key, query and value\n",
    "- Calculate attention scores for Input 1\n",
    "- Calculate softmax\n",
    "- Multiply scores with values\n",
    "- Sum weighted values to get Output 1\n",
    "- Repeat steps 4â€“7 for Input 2 & Input 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u1UxPJlHBVmS"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENdzUZqSBsiB"
   },
   "source": [
    "### Step 1: Prepare inputs\n",
    "\n",
    "For this tutorial, for the shake of simplicity, we start with 3 inputs, each with dimension 4.\n",
    "\n",
    "![texto alternativo](https://miro.medium.com/max/1973/1*hmvdDXrxhJsGhOQClQdkBA.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "Each input here can represent a\n",
    "* word (embedded in 4 dimensions) in a sentence such as \"Tom is home\" for the 3 inputs in this example\n",
    "* snapshot of the 4 dimensions of features (e.g. temperature, pressure, humidity, windspeed) in a day so each input represents a time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "jKYrJsljBhnv",
    "outputId": "7b865905-2151-4a6a-a899-5439aa429af4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 0.],\n",
       "        [0., 2., 0., 2.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [\n",
    "  [1, 0, 1, 0], # Input 1\n",
    "  [0, 2, 0, 2], # Input 2\n",
    "  [1, 1, 1, 1]  # Input 3\n",
    " ]\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Query, Key and Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analogy to search query\n",
    "* How is similarity defined?\n",
    "* What happens to the value in the end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/c508fb4a-28ec-4a3b-8f7f-ec785ee89cdc/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220714%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220714T112539Z&X-Amz-Expires=86400&X-Amz-Signature=0e352e855e82a622f278cb01e7223f95ca0a8df372170b662982a350ea8361a0&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/c673b06b-c0a2-4910-93a2-5d931d06ead1/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220714%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220714T112640Z&X-Amz-Expires=86400&X-Amz-Signature=738f59c528da7f89c25366653af5269f1ed871dbf90d8d7f59c1c502abad279f&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZ96EoE1Bvat"
   },
   "source": [
    "### Step 2: Initialise weights\n",
    "\n",
    "Every input must have three representations (see diagram below). These representations are called **key** (orange), **query** (red), and **value** (purple). For this example, letâ€™s take that we want these representations to have a dimension of 3. Because every input has a dimension of 4, this means each set of the weights must have a shape of 4Ã—3.\n",
    "\n",
    "![texto del enlace](https://miro.medium.com/max/1975/1*VPvXYMGjv0kRuoYqgFvCag.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "jUTNr15JBkSG",
    "outputId": "baa4c379-6174-4990-8cd2-51191e904550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for key: \n",
      " tensor([[0., 0., 1.],\n",
      "        [1., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 1., 0.]])\n",
      "Weights for query: \n",
      " tensor([[1., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 1.]])\n",
      "Weights for value: \n",
      " tensor([[0., 2., 0.],\n",
      "        [0., 3., 0.],\n",
      "        [1., 0., 3.],\n",
      "        [1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "w_key = [\n",
    "  [0, 0, 1],\n",
    "  [1, 1, 0],\n",
    "  [0, 1, 0],\n",
    "  [1, 1, 0]\n",
    "]\n",
    "\n",
    "w_query = [\n",
    "  [1, 0, 1],\n",
    "  [1, 0, 0],\n",
    "  [0, 0, 1],\n",
    "  [0, 1, 1]\n",
    "]\n",
    "\n",
    "w_value = [\n",
    "  [0, 2, 0],\n",
    "  [0, 3, 0],\n",
    "  [1, 0, 3],\n",
    "  [1, 1, 0]\n",
    "]\n",
    "\n",
    "w_key = torch.tensor(w_key, dtype=torch.float32)\n",
    "w_query = torch.tensor(w_query, dtype=torch.float32)\n",
    "w_value = torch.tensor(w_value, dtype=torch.float32)\n",
    "\n",
    "print(\"Weights for key: \\n\", w_key)\n",
    "print(\"Weights for query: \\n\", w_query)\n",
    "print(\"Weights for value: \\n\", w_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pr9XZF9X_Ed"
   },
   "source": [
    "Note: *In a neural network setting, these weights are usually small numbers, initialised randomly using an appropriate random distribution like Gaussian, Xavier and Kaiming distributions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "* These weights are learnt through the network using backpropagation.\n",
    "* An intuitive understanding of the weights represent different semantic aspect we want to focus on for a word embedding by projecting original embeddings into lower dimensional space.\n",
    "    * Different parts of a word embedding would contain different partial semantic 'meanings' of that word e.g. the word \"King\" would have some embedding dimensions indicating royalty and some other dimensions indicating male and yet another for ruler and dominance. Another way these lower dimensional projections can focus on is the grammatical structure such as preposition and location of the tokens/words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxGT5awVB1Xw"
   },
   "source": [
    "### Step 3: Derive key, query and value\n",
    "\n",
    "Now that we have the three sets of weights, letâ€™s actually obtain the **key**, **query** and **value** representations for every input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQwhDIi7aGXp"
   },
   "source": [
    "Obtaining the keys:\n",
    "```\n",
    "               [0, 0, 1]\n",
    "[1, 0, 1, 0]   [1, 1, 0]   [0, 1, 1]\n",
    "[0, 2, 0, 2] x [0, 1, 0] = [4, 4, 0]\n",
    "[1, 1, 1, 1]   [1, 1, 0]   [2, 3, 1]\n",
    "```\n",
    "![texto alternativo](https://miro.medium.com/max/1975/1*dr6NIaTfTxEWzxB2rc0JWg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi0EblXTamFz"
   },
   "source": [
    "Obtaining the values:\n",
    "```\n",
    "               [0, 2, 0]\n",
    "[1, 0, 1, 0]   [0, 3, 0]   [1, 2, 3] \n",
    "[0, 2, 0, 2] x [1, 0, 3] = [2, 8, 0]\n",
    "[1, 1, 1, 1]   [1, 1, 0]   [2, 6, 3]\n",
    "```\n",
    "![texto alternativo](https://miro.medium.com/max/1975/1*5kqW7yEwvcC0tjDOW3Ia-A.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTp2izu1bLNq"
   },
   "source": [
    "Obtaining the querys:\n",
    "```\n",
    "               [1, 0, 1]\n",
    "[1, 0, 1, 0]   [1, 0, 0]   [1, 0, 2]\n",
    "[0, 2, 0, 2] x [0, 0, 1] = [2, 2, 2]\n",
    "[1, 1, 1, 1]   [0, 1, 1]   [2, 1, 3]\n",
    "```\n",
    "![texto alternativo](https://miro.medium.com/max/1975/1*wO_UqfkWkv3WmGQVHvrMJw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qegb9M0KbnRK"
   },
   "source": [
    "Notes: *Notes\n",
    "In practice, a bias vector may be added to the product of matrix multiplication.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "rv2NXynOB7oG",
    "outputId": "a2656b52-4b1d-4726-9d42-522f941b3126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: \n",
      " tensor([[0., 1., 1.],\n",
      "        [4., 4., 0.],\n",
      "        [2., 3., 1.]])\n",
      "Querys: \n",
      " tensor([[1., 0., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 1., 3.]])\n",
      "Values: \n",
      " tensor([[1., 2., 3.],\n",
      "        [2., 8., 0.],\n",
      "        [2., 6., 3.]])\n"
     ]
    }
   ],
   "source": [
    "keys = x @ w_key\n",
    "querys = x @ w_query\n",
    "values = x @ w_value\n",
    "\n",
    "print(\"Keys: \\n\", keys)\n",
    "# tensor([[0., 1., 1.],\n",
    "#         [4., 4., 0.],\n",
    "#         [2., 3., 1.]])\n",
    "\n",
    "print(\"Querys: \\n\", querys)\n",
    "# tensor([[1., 0., 2.],\n",
    "#         [2., 2., 2.],\n",
    "#         [2., 1., 3.]])\n",
    "print(\"Values: \\n\", values)\n",
    "# tensor([[1., 2., 3.],\n",
    "#         [2., 8., 0.],\n",
    "#         [2., 6., 3.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pmf0OQhCnD8"
   },
   "source": [
    "### Step 4: Calculate attention scores\n",
    "![texto alternativo](https://miro.medium.com/max/1973/1*u27nhUppoWYIGkRDmYFN2A.gif)\n",
    "\n",
    "To obtain **attention scores**, we start off with taking a dot product between Input 1â€™s **query** (red) with **all keys** (orange), including itself. Since there are 3 key representations (because we have 3 inputs), we obtain 3 attention scores (blue).\n",
    "\n",
    "```\n",
    "            [0, 4, 2]\n",
    "[1, 0, 2] x [1, 4, 3] = [2, 4, 4]\n",
    "            [1, 0, 1]\n",
    "```\n",
    "Notice that we only use the query from Input 1. Later weâ€™ll work on repeating this same step for the other querys.\n",
    "\n",
    "Note: *The above operation is known as dot product attention, one of the several score functions. Other score functions include scaled dot product and additive/concat.*            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "6GDhKEl0Cokw",
    "outputId": "c91356df-202c-4816-e98d-eefd1e1031d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.,  4.],\n",
      "        [ 4., 16., 12.],\n",
      "        [ 4., 12., 10.]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = querys @ keys.T\n",
    "print(attn_scores)\n",
    "\n",
    "# tensor([[ 2.,  4.,  4.],  # attention scores from Query 1\n",
    "#         [ 4., 16., 12.],  # attention scores from Query 2\n",
    "#         [ 4., 12., 10.]]) # attention scores from Query 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bO3NmnbvCxpX"
   },
   "source": [
    "### Step 5: Calculate softmax\n",
    "![texto alternativo](https://miro.medium.com/max/1973/1*jf__2D8RNCzefwS0TP1Kyg.gif)\n",
    "\n",
    "Take the **softmax** across these **attention scores** (blue).\n",
    "```\n",
    "softmax([2, 4, 4]) = [0.0, 0.5, 0.5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "PDNzdZHVC1ys",
    "outputId": "c528a7be-5c26-46a9-8fdb-1f2b029b6b93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.3379e-02, 4.6831e-01, 4.6831e-01],\n",
      "        [6.0337e-06, 9.8201e-01, 1.7986e-02],\n",
      "        [2.9539e-04, 8.8054e-01, 1.1917e-01]])\n",
      "tensor([[0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 1.0000, 0.0000],\n",
      "        [0.0000, 0.9000, 0.1000]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "attn_scores_softmax = softmax(attn_scores, dim=-1)\n",
    "print(attn_scores_softmax)\n",
    "# tensor([[6.3379e-02, 4.6831e-01, 4.6831e-01],\n",
    "#         [6.0337e-06, 9.8201e-01, 1.7986e-02],\n",
    "#         [2.9539e-04, 8.8054e-01, 1.1917e-01]])\n",
    "\n",
    "# For readability, approximate the above as follows\n",
    "attn_scores_softmax = [\n",
    "  [0.0, 0.5, 0.5],\n",
    "  [0.0, 1.0, 0.0],\n",
    "  [0.0, 0.9, 0.1]\n",
    "]\n",
    "attn_scores_softmax = torch.tensor(attn_scores_softmax)\n",
    "print(attn_scores_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBe71nseDBhb"
   },
   "source": [
    "### Step 6: Multiply scores with values\n",
    "![texto alternativo](https://miro.medium.com/max/1973/1*9cTaJGgXPbiJ4AOCc6QHyA.gif)\n",
    "\n",
    "The softmaxed attention scores for each input (blue) is multiplied with its corresponding **value** (purple). This results in 3 alignment vectors (yellow). In this tutorial, weâ€™ll refer to them as **weighted values**.\n",
    "```\n",
    "1: 0.0 * [1, 2, 3] = [0.0, 0.0, 0.0]\n",
    "2: 0.5 * [2, 8, 0] = [1.0, 4.0, 0.0]\n",
    "3: 0.5 * [2, 6, 3] = [1.0, 3.0, 1.5]\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "id": "tNnx-Fx5DFDi",
    "outputId": "abc7a8ec-f964-483a-9bfb-2848f0e8e592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 4.0000, 0.0000],\n",
      "         [2.0000, 8.0000, 0.0000],\n",
      "         [1.8000, 7.2000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 3.0000, 1.5000],\n",
      "         [0.0000, 0.0000, 0.0000],\n",
      "         [0.2000, 0.6000, 0.3000]]])\n"
     ]
    }
   ],
   "source": [
    "weighted_values = values[:,None] * attn_scores_softmax.T[:,:,None]\n",
    "print(weighted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gU6w0U9ADQIc"
   },
   "source": [
    "### Step 7: Sum weighted values\n",
    "![texto alternativo](https://miro.medium.com/max/1973/1*1je5TwhVAwwnIeDFvww3ew.gif)\n",
    "\n",
    "Take all the **weighted values** (yellow) and sum them element-wise:\n",
    "\n",
    "```\n",
    "  [0.0, 0.0, 0.0]\n",
    "+ [1.0, 4.0, 0.0]\n",
    "+ [1.0, 3.0, 1.5]\n",
    "-----------------\n",
    "= [2.0, 7.0, 1.5]\n",
    "```\n",
    "\n",
    "The resulting vector ```[2.0, 7.0, 1.5]``` (dark green) **is Output 1**, which is based on the **query representation from Input 1** interacting with all other keys, including itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "* In effect we have applied contextual weightings (normalised) of each of the 3 words/tokens onto their corresponding value vectors before summing them to arrive at the final attention vectors.\n",
    "* This contextual weights are derived from the similarity the current query is with each of the keys (including the current self-key) in a normalised fashion\n",
    "* So the final attention vector for the current token is the weighted sum of the value vectors where weighting is dependent on the similarity of the current query with each of the keys. The higher the similarity of the current query with a key K, the more the value V of that token would be weighted in arriving at the attention vector of the current token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3yNYDUEgAos"
   },
   "source": [
    "### Step 8: Repeat for Input 2 & Input 3\n",
    "![texto alternativo](https://miro.medium.com/max/1973/1*G8thyDVqeD8WHim_QzjvFg.gif)\n",
    "\n",
    "Note: *The dimension of **query** and **key** must always be the same because of the dot product score function. However, the dimension of **value** may be different from **query** and **key**. The resulting output will consequently follow the dimension of **value**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "R6excNSUDRRj",
    "outputId": "e5161fbe-05a5-41d2-da1e-5951ce8b1674"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 7.0000, 1.5000],\n",
      "        [2.0000, 8.0000, 0.0000],\n",
      "        [2.0000, 7.8000, 0.3000]])\n"
     ]
    }
   ],
   "source": [
    "outputs = weighted_values.sum(dim=0)\n",
    "print(outputs)\n",
    "\n",
    "# tensor([[2.0000, 7.0000, 1.5000],  # Output 1\n",
    "#         [2.0000, 8.0000, 0.0000],  # Output 2\n",
    "#         [2.0000, 7.8000, 0.3000]]) # Output 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oavQirdbhAK7"
   },
   "source": [
    "### Bonus: Tensorflow 2 implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "575q0u_ahP-6",
    "outputId": "867a4e88-2223-41e4-ccd5-dbc47f580c83"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "0vjwwEKMhqmZ",
    "outputId": "56e5ed58-e100-434d-a8b2-00325bfc0d40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "tf.Tensor(\n",
      "[[1. 0. 1. 0.]\n",
      " [0. 2. 0. 2.]\n",
      " [1. 1. 1. 1.]], shape=(3, 4), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 16:22:43.324237: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-14 16:22:43.324532: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "x = [\n",
    "  [1, 0, 1, 0], # Input 1\n",
    "  [0, 2, 0, 2], # Input 2\n",
    "  [1, 1, 1, 1]  # Input 3\n",
    " ]\n",
    "\n",
    "x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "TN-pri7rhwJ-",
    "outputId": "aa8b1395-80a3-41e1-b544-beb06ce65a96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for key: \n",
      " tf.Tensor(\n",
      "[[0. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 1. 0.]], shape=(4, 3), dtype=float32)\n",
      "Weights for query: \n",
      " tf.Tensor(\n",
      "[[1. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 1.]], shape=(4, 3), dtype=float32)\n",
      "Weights for value: \n",
      " tf.Tensor(\n",
      "[[0. 2. 0.]\n",
      " [0. 3. 0.]\n",
      " [1. 0. 3.]\n",
      " [1. 1. 0.]], shape=(4, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w_key = [\n",
    "  [0, 0, 1],\n",
    "  [1, 1, 0],\n",
    "  [0, 1, 0],\n",
    "  [1, 1, 0]\n",
    "]\n",
    "\n",
    "w_query = [\n",
    "  [1, 0, 1],\n",
    "  [1, 0, 0],\n",
    "  [0, 0, 1],\n",
    "  [0, 1, 1]\n",
    "]\n",
    "\n",
    "w_value = [\n",
    "  [0, 2, 0],\n",
    "  [0, 3, 0],\n",
    "  [1, 0, 3],\n",
    "  [1, 1, 0]\n",
    "]\n",
    "\n",
    "w_key = tf.convert_to_tensor(w_key, dtype=tf.float32)\n",
    "w_query = tf.convert_to_tensor(w_query, dtype=tf.float32)\n",
    "w_value = tf.convert_to_tensor(w_value, dtype=tf.float32)\n",
    "\n",
    "print(\"Weights for key: \\n\", w_key)\n",
    "print(\"Weights for query: \\n\", w_query)\n",
    "print(\"Weights for value: \\n\", w_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "Jp2DP46Sh19r",
    "outputId": "5c1befaf-e096-454c-8402-885f049752e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1.]\n",
      " [4. 4. 0.]\n",
      " [2. 3. 1.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 0. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 1. 3.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [2. 8. 0.]\n",
      " [2. 6. 3.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "keys = tf.matmul(x, w_key)\n",
    "querys = tf.matmul(x, w_query)\n",
    "values = tf.matmul(x, w_value)\n",
    "\n",
    "print(keys)\n",
    "print(querys)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "tLJDo_bFigkm",
    "outputId": "b5d8e02d-9531-49c8-a587-7a6e0b6f884d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2.  4.  4.]\n",
      " [ 4. 16. 12.]\n",
      " [ 4. 12. 10.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "attn_scores = tf.matmul(querys, keys, transpose_b=True)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "8QY858MEiibV",
    "outputId": "2e84f48b-a4ed-4116-8655-21cbb9de8358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6.3378938e-02 4.6831056e-01 4.6831056e-01]\n",
      " [6.0336674e-06 9.8200792e-01 1.7986100e-02]\n",
      " [2.9538720e-04 8.8053685e-01 1.1916769e-01]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.  0.5 0.5]\n",
      " [0.  1.  0. ]\n",
      " [0.  0.9 0.1]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "attn_scores_softmax = tf.nn.softmax(attn_scores, axis=-1)\n",
    "print(attn_scores_softmax)\n",
    "\n",
    "# For readability, approximate the above as follows\n",
    "# Alternatively use numpy to round\n",
    "# attn_scores_softmax = tf.convert_to_tensor(np.round(attn_scores_softmax, 1))\n",
    "attn_scores_softmax = [\n",
    "  [0.0, 0.5, 0.5],\n",
    "  [0.0, 1.0, 0.0],\n",
    "  [0.0, 0.9, 0.1]\n",
    "]\n",
    "\n",
    "attn_scores_softmax = tf.convert_to_tensor(attn_scores_softmax)\n",
    "print(attn_scores_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "TOJMfkFpi0KQ",
    "outputId": "8de18989-50d7-4534-cf5c-2711c66d17ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.  0.  0. ]\n",
      "  [0.  0.  0. ]\n",
      "  [0.  0.  0. ]]\n",
      "\n",
      " [[1.  4.  0. ]\n",
      "  [2.  8.  0. ]\n",
      "  [1.8 7.2 0. ]]\n",
      "\n",
      " [[1.  3.  1.5]\n",
      "  [0.  0.  0. ]\n",
      "  [0.2 0.6 0.3]]], shape=(3, 3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "weighted_values = values[:,None] * tf.transpose(attn_scores_softmax)[:,:,None]\n",
    "print(weighted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "jan_cyy7i-s7",
    "outputId": "09b1406f-3a08-47e2-8dee-d4d6334ef1de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2.        7.        1.5      ]\n",
      " [2.        8.        0.       ]\n",
      " [2.        7.7999997 0.3      ]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "outputs = tf.reduce_sum(weighted_values, axis=0)  # 6\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "basic_self-attention .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
